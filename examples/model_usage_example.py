#!/usr/bin/env python3
"""
ç®€åŒ–æ¨¡å‹è°ƒç”¨ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ–°çš„ç®€åŒ–æ¨¡å‹è°ƒç”¨æ¥å£
"""

import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from toolace.utils.model_manager import get_model_generator, generate


def main():
    """ä¸»ç¤ºä¾‹å‡½æ•°"""
    print("ğŸš€ ToolACE ç®€åŒ–æ¨¡å‹è°ƒç”¨ç¤ºä¾‹")
    
    # ç¤ºä¾‹1: ä½¿ç”¨å·¥å‚å‡½æ•°è·å–æ¨¡å‹ç”Ÿæˆå™¨
    print("\nğŸ“Š ç¤ºä¾‹1: ä½¿ç”¨å·¥å‚å‡½æ•°åŠ¨æ€è·å–æ¨¡å‹")
    
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªAPIè®¾è®¡ä¸“å®¶ï¼Œè¯·å¸®åŠ©è®¾è®¡APIæ¥å£ã€‚"
    user_prompt = "è¯·è®¾è®¡ä¸€ä¸ªå¤©æ°”æŸ¥è¯¢çš„APIæ¥å£"
    
    # è·å–Mockæ¨¡å‹ç”Ÿæˆå™¨
    try:
        print("\nğŸ­ ä½¿ç”¨Mockæ¨¡å‹:")
        mock_generator = get_model_generator("mock_llm")
        response = mock_generator(system_prompt, user_prompt, temperature=0.7)
        print(f"å“åº”: {response[:200]}...")
    except Exception as e:
        print(f"Mockæ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
    
    # è·å–Qwenæ¨¡å‹ç”Ÿæˆå™¨  
    try:
        print("\nğŸ¤– ä½¿ç”¨Qwen3 32Bæ¨¡å‹:")
        qwen_generator = get_model_generator("qwen3_32b")
        response = qwen_generator(
            system_prompt, 
            user_prompt, 
            temperature=0.3,
            max_tokens=512
        )
        print(f"å“åº”: {response[:200]}...")
    except Exception as e:
        print(f"Qwenæ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
    
    # ç¤ºä¾‹2: ç›´æ¥ä½¿ç”¨ç»Ÿä¸€ç”Ÿæˆå‡½æ•°
    print("\nğŸ“Š ç¤ºä¾‹2: ä½¿ç”¨ç»Ÿä¸€ç”Ÿæˆå‡½æ•°")
    
    try:
        print("\nğŸ”§ ç›´æ¥è°ƒç”¨ç”Ÿæˆå‡½æ•°:")
        response = generate(
            model_key="mock_llm",
            system_prompt="ä½ æ˜¯ä¸€ä¸ªå¯¹è¯ç”Ÿæˆä¸“å®¶ã€‚",
            user_prompt="è¯·ç”Ÿæˆä¸€ä¸ªå…³äºå·¥å…·è°ƒç”¨çš„ç®€å•ç¤ºä¾‹",
            temperature=0.8,
            max_tokens=300
        )
        print(f"å“åº”: {response}")
        
    except Exception as e:
        print(f"ç»Ÿä¸€ç”Ÿæˆå‡½æ•°è°ƒç”¨å¤±è´¥: {e}")
    
    # ç¤ºä¾‹3: ç›´æ¥å¯¼å…¥æ¨¡å‹æ¨¡å—ä½¿ç”¨
    print("\nğŸ“Š ç¤ºä¾‹3: ç›´æ¥å¯¼å…¥æ¨¡å‹æ¨¡å—")
    
    try:
        from toolace.utils.model_generator import mock_llm
        print("\nğŸ¯ ç›´æ¥å¯¼å…¥mock_llmæ¨¡å—:")
        response = mock_llm.generate(
            system="ä½ æ˜¯ä¸€ä¸ªæ•…äº‹è®²è¿°è€…ã€‚",
            user="è¯·è®²ä¸€ä¸ªå…³äºAIçš„ç®€çŸ­æ•…äº‹",
            temperature=0.9
        )
        print(f"ç”Ÿæˆçš„æ•…äº‹: {response}")
        
    except Exception as e:
        print(f"ç›´æ¥å¯¼å…¥è°ƒç”¨å¤±è´¥: {e}")
    
    # ç¤ºä¾‹4: æµå¼ç”Ÿæˆ
    print("\nğŸ“Š ç¤ºä¾‹4: æµå¼ç”Ÿæˆ")
    
    try:
        from toolace.utils.model_manager import stream_generate
        print("\nğŸŒŠ æµå¼ç”Ÿæˆç¤ºä¾‹:")
        
        stream = stream_generate(
            model_key="mock_llm",
            system_prompt="ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹åŠ©æ‰‹ã€‚",
            user_prompt="è¯·è§£é‡Šä»€ä¹ˆæ˜¯å‡½æ•°è°ƒç”¨",
            temperature=0.7
        )
        
        print("æµå¼è¾“å‡º: ", end="")
        for chunk in stream:
            print(chunk, end="", flush=True)
        print()
        
    except Exception as e:
        print(f"æµå¼ç”Ÿæˆå¤±è´¥: {e}")
    
    print("\nğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
    print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print("1. ä½¿ç”¨ get_model_generator(model_name) è·å–æ¨¡å‹ç”Ÿæˆå‡½æ•°")
    print("2. ä½¿ç”¨ generate(model_name, system, user, **kwargs) ç»Ÿä¸€è°ƒç”¨")
    print("3. ç›´æ¥å¯¼å…¥å…·ä½“æ¨¡å‹æ¨¡å—ä½¿ç”¨")
    print("4. æ‰€æœ‰æ¨¡å‹éƒ½æ”¯æŒ generate(system, user, **kwargs) æ¥å£")


if __name__ == "__main__":
    main()
